# gRINN Web Service Environment Configuration
# Copy this file to .env and customize for your deployment

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================
# For local development, use SQLite (no setup required)
DATABASE_URL=sqlite:////tmp/grinn_dev.db

# For production, use PostgreSQL
# DATABASE_URL=postgresql://grinn_user:your_password@localhost:5432/grinn_web

# =============================================================================
# LOCAL STORAGE CONFIGURATION
# =============================================================================
# Path on HOST machine to store job files (uploaded files, results, temp files)
# This path is mounted into Docker containers at /data/grinn-jobs
# For multi-worker setups, use an NFS mount accessible from all workers
STORAGE_PATH=/data/grinn-jobs

# Job file retention period in hours (default: 72 = 3 days)
# After this period, job files are deleted and status changes to EXPIRED
JOB_FILE_RETENTION_HOURS=72

# How long to keep expired job records in the database (default: 30 days)
# After this period, expired jobs are permanently deleted from the database
EXPIRED_JOB_RETENTION_DAYS=30

# How often the cleanup task runs in seconds (default: 21600 = 6 hours)
# Set lower for testing (e.g., 30 for 30 seconds)
CLEANUP_INTERVAL_SECONDS=21600

# =============================================================================
# WORKER REGISTRATION
# =============================================================================
# Token for worker authentication (generate with: python -c "import secrets; print(secrets.token_urlsafe(32))")
# WORKER_REGISTRATION_TOKEN=your-secure-token-here

# Worker facility name
WORKER_FACILITY=main-facility

# =============================================================================
# REDIS CONFIGURATION
# =============================================================================
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
# For password-protected Redis (optional)
# REDIS_PASSWORD=your-redis-password
CELERY_BROKER_URL=redis://localhost:6379/0
CELERY_RESULT_BACKEND=redis://localhost:6379/0

# =============================================================================
# APPLICATION CONFIGURATION
# =============================================================================
# Backend API server
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8050

# Backend API public URL (proxied through nginx)
BACKEND_PUBLIC_URL=https://grinn.bio-cloud.site/api

# Dashboard public URL template (proxied through nginx)
DASHBOARD_PUBLIC_URL_TEMPLATE=https://grinn.bio-cloud.site/dashboard/{port}

# Frontend dashboard
FRONTEND_HOST=0.0.0.0
FRONTEND_PORT=8051
FRONTEND_DEBUG=false

# Public hostname/IP for client-facing URLs (e.g., download links, dashboard iframes)
# If not set, defaults to the system hostname (socket.gethostname())
# For remote access, set this to your server's public IP or domain name
# PUBLIC_HOST=your-server.example.com

# =============================================================================
# GRINN DOCKER CONFIGURATION
# =============================================================================
GRINN_DOCKER_IMAGE=grinn:gromacs-2024.1
DOCKER_TIMEOUT=3600

# =============================================================================
# GRINN DASHBOARD DOCKER CONFIGURATION
# =============================================================================
# Docker image for interactive dashboard visualization
DASHBOARD_DOCKER_IMAGE=grinn-dashboard:latest

# Port range for dashboard instances (one port per active dashboard)
DASHBOARD_PORT_START=8100
DASHBOARD_PORT_END=8200

# Maximum lifetime for any dashboard container (seconds, safety limit)
# After this time, dashboard is stopped regardless of activity
# Set to 0 to disable (not recommended for production)
DASHBOARD_TIMEOUT=3600

# Maximum concurrent dashboard instances
# When this limit is reached, "Launch Dashboard" buttons will be disabled
# and users will be directed to download results and use standalone gRINN
DASHBOARD_MAX_INSTANCES=10

# Idle timeout for dashboard cleanup (minutes)
# Dashboards without heartbeats for this duration will be stopped
DASHBOARD_IDLE_TIMEOUT_MINUTES=5

# How often to check for idle dashboards (seconds)
DASHBOARD_CLEANUP_INTERVAL_SECONDS=60

# Dashboard heartbeat interval - how often dashboards report they're active (seconds)
DASHBOARD_HEARTBEAT_INTERVAL_SECONDS=30

# Public hostname/IP for dashboard URLs (used in dashboard iframe links)
# If not set, defaults to PUBLIC_HOST (or system hostname if PUBLIC_HOST is also not set)
# DASHBOARD_PUBLIC_HOST=your-server.example.com

# Full URL template for dashboard instances (for reverse proxy setups)
# Use {job_id} as placeholder for the job ID
# Example: https://example.com/api/dashboard/{job_id} (backend proxies to dashboard container)
# If not set, defaults to http://{DASHBOARD_PUBLIC_HOST}:{port} (direct port access)
# DASHBOARD_PUBLIC_URL_TEMPLATE=https://your-server.example.com/api/dashboard/{job_id}

# =============================================================================
# AI CHATBOT CONFIGURATION
# =============================================================================
# API keys for dashboard chatbot (at least one required for chatbot functionality)
# Gemini API key (Google AI)
GEMINI_API_KEY=
# Google API key (alternative to Gemini, supports same models)
GOOGLE_API_KEY=
# Anthropic API key (for Claude models)
ANTHROPIC_API_KEY=
# Available LLM models shown in the chatbot dropdown.
# Accepts JSON array, comma-separated, or newline-separated values.
# Examples:
# PANDASAI_MODELS=["gemini/gemini-pro-latest","gemini/gemini-1.5-pro-latest"]
# PANDASAI_MODELS=gemini/gemini-pro-latest,gemini/gemini-1.5-pro-latest
PANDASAI_MODELS=

# Default selection for the dropdown (optional).
# If set, it will be auto-inserted into PANDASAI_MODELS if missing.
PANDASAI_DEFAULT_MODEL=

# Backward-compatible single-model setting (still supported).
# PANDASAI_MODEL=gemini/gemini-pro-latest

# Token budget per dashboard session (chatbot). When exhausted, queries are blocked.
# Set to 0 or leave empty for unlimited.
PANDASAI_TOKEN_LIMIT=1000

# Enable/disable Docker sandbox for chatbot code execution.
# When true (default for standalone), LLM-generated code runs in isolated Docker container.
# When false, code executes locally (used for dashboard containers where Docker-in-Docker unavailable).
# WARNING: Disabling sandbox executes LLM-generated code without isolation - only use in trusted environments.
PANDASAI_USE_DOCKER_SANDBOX=true

# =============================================================================
# FILE UPLOAD LIMITS
# =============================================================================
# Hard limit for trajectory-class files (XTC/TRR in trajectory mode, PDB in ensemble mode)
MAX_TRAJECTORY_FILE_SIZE_MB=100

# Hard limit for structure/topology files (PDB in trajectory mode, GRO, TOP, TPR, ITP, etc.)
MAX_OTHER_FILE_SIZE_MB=10

# Optional global cap: fail preflight if exceeded
# Applies to trajectory frames (XTC/TRR) and ensemble PDB models
MAX_FRAMES=

# =============================================================================
# CAPACITY LIMITS
# =============================================================================
# Maximum concurrent jobs running across all workers (global limit)
MAX_CONCURRENT_JOBS=10

# Maximum jobs that can be queued (waiting to be processed) at once
# New submissions will be rejected with 503 when this limit is reached
MAX_QUEUED_JOBS=50

# Maximum concurrent jobs per worker (worker-specific limit)
# Workers can declare different capacities during registration
WORKER_MAX_CONCURRENT_JOBS=2

# Worker heartbeat settings for health monitoring
WORKER_HEARTBEAT_INTERVAL_SECONDS=30
WORKER_HEARTBEAT_TIMEOUT_SECONDS=90

# =============================================================================
# SECURITY
# =============================================================================
# Generate secure random strings for production:
#   python -c "import secrets; print(secrets.token_urlsafe(32))"
SECRET_KEY=dev-secret-key-change-in-production

# Admin API key for privileged endpoints (worker management, cleanup triggers)
# Generate with: python -c "import secrets; print(secrets.token_urlsafe(32))"
# ADMIN_API_KEY=your-admin-api-key-here

# =============================================================================
# GROMACS CONFIGURATION
# =============================================================================
# Default GROMACS version shown in dropdown (must match an available Docker image)
# Available versions: 2020.7, 2021.1-2021.7, 2022.1-2022.6, 2023.1-2023.5, 2024.1-2024.5, 2025.1-2025.2
DEFAULT_GROMACS_VERSION=2024.1

# =============================================================================
# EXAMPLE DATA
# =============================================================================
# Example data paths for "Load Example Data" button (mode-specific)
# Trajectory mode: Should contain structure (.pdb/.gro), trajectory (.xtc/.trr), and topology (.top/.tpr) files
EXAMPLE_DATA_PATH_TRAJECTORY=/path/to/trajectory_example_data

# Ensemble mode: Should contain a single multi-model PDB file with MODEL/ENDMDL records
EXAMPLE_DATA_PATH_ENSEMBLE=/path/to/ensemble_example_data